{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title üöÄ Install dependencies\n",
        "!pip -q install faiss-cpu sentence-transformers pypdf beautifulsoup4 lxml requests gradio pillow reportlab\n",
        "!pip -q install transformers accelerate --upgrade\n",
        "# Optional clients; installed but not required unless you use them\n",
        "!pip -q install groq openai\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL0XA8AWDZU1",
        "outputId": "cf2bf7f6-8685-410f-db59-88a3372486fc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m893.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üîß Imports & helpers\n",
        "import os, io, json, math, time, textwrap, shutil, string, uuid\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Tuple\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pypdf import PdfReader\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.utils import ImageReader\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "# Hugging Face Transformers (for local, small model)\n",
        "from transformers import pipeline\n",
        "\n",
        "WORKDIR = Path(\"/content/rag_colab\")\n",
        "WORKDIR.mkdir(parents=True, exist_ok=True)\n",
        "ASSETS = WORKDIR / \"assets\"\n",
        "ASSETS.mkdir(exist_ok=True)\n",
        "\n",
        "def slug(s, n=50):\n",
        "    safe = \"\".join(ch for ch in s.lower() if ch.isalnum() or ch in \"-_ \")\n",
        "    safe = \"-\".join(safe.split())\n",
        "    return safe[:n] or str(uuid.uuid4())\n",
        "\n",
        "def chunk_text(text:str, max_tokens:int=220, overlap:int=40) -> List[str]:\n",
        "    \"\"\"\n",
        "    Simple word-based chunker approximating token sizes.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        chunk = words[i: i + max_tokens]\n",
        "        chunks.append(\" \".join(chunk))\n",
        "        i += max_tokens - overlap\n",
        "        if i <= 0: break\n",
        "    return chunks\n",
        "\n",
        "def read_pdf_text(pdf_path: str) -> str:\n",
        "    reader = PdfReader(pdf_path)\n",
        "    pages = []\n",
        "    for p in reader.pages:\n",
        "        try:\n",
        "            pages.append(p.extract_text() or \"\")\n",
        "        except:\n",
        "            pages.append(\"\")\n",
        "    return \"\\n\".join(pages)\n",
        "\n",
        "def scrape_url(url: str) -> str:\n",
        "    r = requests.get(url, timeout=15)\n",
        "    r.raise_for_status()\n",
        "    soup = BeautifulSoup(r.text, \"lxml\")\n",
        "    # Remove script/style\n",
        "    for tag in soup([\"script\",\"style\",\"noscript\"]): tag.extract()\n",
        "    text = soup.get_text(separator=\" \")\n",
        "    text = \" \".join(text.split())\n",
        "    return text\n",
        "\n",
        "def normalize(vecs: np.ndarray) -> np.ndarray:\n",
        "    norms = np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-12\n",
        "    return vecs / norms\n",
        "\n",
        "def wrap_for_image(s, width=80):\n",
        "    return \"\\n\".join(textwrap.wrap(s, width=width, replace_whitespace=False))\n",
        "\n",
        "def text_to_image(text:str, out_path:Path, title:str=None, width=1200, padding=30):\n",
        "    \"\"\"\n",
        "    Render long text to a PNG for use as 'screenshot' in the report.\n",
        "    \"\"\"\n",
        "    font = ImageFont.load_default()\n",
        "    lines = (title + \"\\n\\n\" if title else \"\") + wrap_for_image(text, width=110)\n",
        "    dummy = Image.new(\"RGB\", (width, 100), \"white\")\n",
        "    draw = ImageDraw.Draw(dummy)\n",
        "    w, h = draw.multiline_textsize(lines, font=font)\n",
        "    img = Image.new(\"RGB\", (width, h + padding*2), \"white\")\n",
        "    draw = ImageDraw.Draw(img)\n",
        "    draw.multiline_text((padding, padding), lines, fill=\"black\", font=font)\n",
        "    img.save(out_path)\n"
      ],
      "metadata": {
        "id": "c_jT4rrgDZYw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ‚öôÔ∏è Configuration\n",
        "# Choose one: \"local_flan_t5\" (no API needed), \"groq\", \"openai\", \"hf_inference\"\n",
        "LLM_BACKEND = \"groq\"  #@param [\"local_flan_t5\", \"groq\", \"openai\", \"hf_inference\"]\n",
        "\n",
        "# API keys (only needed if using groq/openai/hf_inference)\n",
        "GROQ_API_KEY = \"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\"  #@param {type:\"string\"}\n",
        "OPENAI_API_KEY = \"\"  #@param {type:\"string\"}\n",
        "HF_INFERENCE_TOKEN = \"\"  #@param {type:\"string\"}\n",
        "\n",
        "# Models\n",
        "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\" # small, fast, free\n",
        "# Local lightweight model (works on CPU in Colab; concise answers)\n",
        "LOCAL_GENERATION_MODEL = \"google/flan-t5-base\"  # small, reliable demo\n",
        "# Groq model example (fast, free tier exists, needs key)\n",
        "GROQ_MODEL = \"llama-3.1-8b-instant\"\n",
        "# OpenAI example\n",
        "OPENAI_MODEL = \"gpt-4o-mini\"\n",
        "# HF Inference example\n",
        "HF_INFERENCE_MODEL = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "\n",
        "TOP_K = 4  # retrieval size\n",
        "MAX_CHUNK_TOKENS = 220\n",
        "CHUNK_OVERLAP = 40\n",
        "\n",
        "# Output paths\n",
        "DB_META_JSON = WORKDIR / \"db_meta.json\"\n",
        "FAISS_INDEX_PATH = str(WORKDIR / \"faiss_index.index\")\n",
        "REPORT_PDF = str(WORKDIR / \"RAG_Project_Report.pdf\")\n",
        "\n",
        "print(f\"LLM backend: {LLM_BACKEND}\")\n",
        "print(f\"Workdir: {WORKDIR}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28DkiMvMDZb3",
        "outputId": "e0c076f2-c7dd-4e8b-f393-8e28515d5392"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM backend: local_flan_t5\n",
            "Workdir: /content/rag_colab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx\n",
        "\n",
        "from docx import Document\n",
        "from docx.shared import Inches\n",
        "from pathlib import Path\n",
        "\n",
        "# Save path\n",
        "REPORT_DOCX = \"/content/RAG_Project_Report.docx\"\n",
        "\n",
        "# Create document\n",
        "doc = Document()\n",
        "\n",
        "# ---------------- Cover Page ----------------\n",
        "doc.add_heading(\"Retrieval-Augmented Generation (RAG) Project\", 0)\n",
        "doc.add_paragraph(\"Final Project Report\\n\\n\")\n",
        "doc.add_paragraph(\"Prepared by: Your Name\\nSubmission Date: 20th August 2025\")\n",
        "doc.add_page_break()\n",
        "\n",
        "# ---------------- Tools & Libraries ----------------\n",
        "doc.add_heading(\"Tools & Libraries\", level=1)\n",
        "doc.add_paragraph(\"\"\"\n",
        "- LLMs: Hugging Face Transformers (Flan-T5), Groq API, OpenAI API, Hugging Face Inference API\n",
        "- Vector Database: FAISS\n",
        "- Embeddings: Sentence-Transformers (all-MiniLM-L6-v2)\n",
        "- Document Handling: pypdf (for PDFs), BeautifulSoup + requests (for scraping)\n",
        "- UI: Gradio\n",
        "- Reporting: Pillow, ReportLab, python-docx\n",
        "\"\"\")\n",
        "\n",
        "# ---------------- Step-by-Step Process ----------------\n",
        "doc.add_heading(\"Step-by-Step Process\", level=1)\n",
        "doc.add_paragraph(\"\"\"\n",
        "1. Data Loading:\n",
        "   Loaded data from custom text, PDFs, and optional scraped content.\n",
        "2. Chunking:\n",
        "   Split text into ~220-token chunks with overlap.\n",
        "3. Embeddings:\n",
        "   Generated embeddings using Sentence-Transformers.\n",
        "4. Vector Database:\n",
        "   Stored embeddings inside FAISS for efficient similarity search.\n",
        "5. Retrieval:\n",
        "   Queried Top-K most relevant chunks using cosine similarity.\n",
        "6. Prompt Construction:\n",
        "   Built augmented prompt with retrieved chunks.\n",
        "7. Generation:\n",
        "   Passed prompt to chosen LLM backend (Flan-T5, Groq, OpenAI, or HF API).\n",
        "8. UI:\n",
        "   Provided Gradio interface for user interaction.\n",
        "9. Report:\n",
        "   Auto-generated project report with explanations and screenshots.\n",
        "\"\"\")\n",
        "\n",
        "# ---------------- Screenshots ----------------\n",
        "assets = Path(\"/content/rag_colab/assets\")\n",
        "for name, title in [\n",
        "    (\"db_creation.png\", \"Database Creation & Insertion\"),\n",
        "    (\"retrieval.png\", \"Retrieval Results\"),\n",
        "    (\"final_answer.png\", \"Final Answer\"),\n",
        "]:\n",
        "    img_path = assets / name\n",
        "    if img_path.exists():\n",
        "        doc.add_heading(title, level=2)\n",
        "        doc.add_picture(str(img_path), width=Inches(5.5))\n",
        "        doc.add_paragraph(f\"Figure: {title}\")\n",
        "\n",
        "# Save file\n",
        "doc.save(REPORT_DOCX)\n",
        "print(\"‚úÖ Word Report generated at:\", REPORT_DOCX)\n"
      ],
      "metadata": {
        "id": "SYwyT-I1E90r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üì• Load your data (edit as you like)\n",
        "\n",
        "# Option A: Raw text samples (you can paste your own here)\n",
        "raw_texts = [\n",
        "    \"\"\"Taaza Paidawar Markaz is a fictional e-commerce idea focused on fresh produce delivery across Pakistan.\n",
        "    It aims to connect farmers directly to consumers with transparent pricing and quality guarantees.\"\"\",\n",
        "    \"\"\"RAG (Retrieval Augmented Generation) improves LLM answers by retrieving relevant context from a vector database\n",
        "    and passing it as prompt context to the generator model.\"\"\",\n",
        "]\n",
        "\n",
        "# Option B: Upload PDFs via Colab file picker (uncomment to use)\n",
        "# from google.colab import files\n",
        "# uploaded = files.upload()\n",
        "# for fn in uploaded:\n",
        "#     pdf_text = read_pdf_text(fn)\n",
        "#     raw_texts.append(pdf_text)\n",
        "\n",
        "# Option C: Scrape a URL (keep it small & public; or add multiple)\n",
        "# try:\n",
        "#     raw_texts.append(scrape_url(\"https://www.python.org/about/\"))\n",
        "# except Exception as e:\n",
        "#     print(\"Scrape failed (skipping):\", e)\n",
        "\n",
        "print(f\"Loaded {len(raw_texts)} source documents.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrHZ-Nv0DZpF",
        "outputId": "40997169-f2cb-4ab2-d76d-c1cf2f9b6592"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 2 source documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üîé Retrieval function (top-K by cosine similarity)\n",
        "# Reload (useful if you come back later in the notebook)\n",
        "index = faiss.read_index(FAISS_INDEX_PATH)\n",
        "with open(DB_META_JSON) as f:\n",
        "    meta = json.load(f)\n",
        "docs = meta[\"docs\"]\n",
        "\n",
        "def retrieve(query: str, top_k=TOP_K) -> List[Dict]:\n",
        "    q_emb = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
        "    scores, idxs = index.search(q_emb, top_k)\n",
        "    results = []\n",
        "    for score, idx in zip(scores[0], idxs[0]):\n",
        "        d = docs[idx].copy()\n",
        "        d[\"score\"] = float(score)\n",
        "        results.append(d)\n",
        "    return results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "1WppDjLxDow4",
        "outputId": "c31a5073-a1e4-4e28-ce94-9190f58e6601"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Error in faiss::FileIOReader::FileIOReader(const char*) at /project/third-party/faiss/faiss/impl/io.cpp:69: Error: 'f' failed: could not open /content/rag_colab/faiss_index.index for reading: No such file or directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1458596576.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#@title üîé Retrieval function (top-K by cosine similarity)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Reload (useful if you come back later in the notebook)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFAISS_INDEX_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDB_META_JSON\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/faiss/swigfaiss_avx2.py\u001b[0m in \u001b[0;36mread_index\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m  11773\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11774\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 11775\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_swigfaiss_avx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  11776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  11777\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_index_binary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error in faiss::FileIOReader::FileIOReader(const char*) at /project/third-party/faiss/faiss/impl/io.cpp:69: Error: 'f' failed: could not open /content/rag_colab/faiss_index.index for reading: No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ü§ñ LLM wrappers\n",
        "from typing import Optional\n",
        "\n",
        "# Local pipeline (no key needed)\n",
        "_local_pipe = None\n",
        "def local_generate(prompt: str, max_new_tokens=256) -> str:\n",
        "    global _local_pipe\n",
        "    if _local_pipe is None:\n",
        "        _local_pipe = pipeline(\"text2text-generation\", model=LOCAL_GENERATION_MODEL, device_map=\"auto\")\n",
        "    out = _local_pipe(prompt, max_new_tokens=max_new_tokens, do_sample=False)[0][\"generated_text\"]\n",
        "    return out.strip()\n",
        "\n",
        "# Groq\n",
        "def groq_generate(prompt: str, max_tokens=400) -> str:\n",
        "    if not os.getenv(\"GROQ_API_KEY\") and not GROQ_API_KEY:\n",
        "        raise RuntimeError(\"Set GROQ_API_KEY to use Groq.\")\n",
        "    from groq import Groq\n",
        "    client = Groq(api_key=GROQ_API_KEY or os.getenv(\"GROQ_API_KEY\"))\n",
        "    resp = client.chat.completions.create(\n",
        "        model=GROQ_MODEL,\n",
        "        messages=[{\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},\n",
        "                  {\"role\":\"user\",\"content\":prompt}],\n",
        "        temperature=0.2,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "# OpenAI\n",
        "def openai_generate(prompt: str, max_tokens=400) -> str:\n",
        "    if not os.getenv(\"OPENAI_API_KEY\") and not OPENAI_API_KEY:\n",
        "        raise RuntimeError(\"Set OPENAI_API_KEY to use OpenAI.\")\n",
        "    from openai import OpenAI\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY or os.getenv(\"OPENAI_API_KEY\"))\n",
        "    resp = client.chat.completions.create(\n",
        "        model=OPENAI_MODEL,\n",
        "        messages=[{\"role\":\"system\",\"content\":\"You are a helpful assistant.\"},\n",
        "                  {\"role\":\"user\",\"content\":prompt}],\n",
        "        temperature=0.2,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "    return resp.choices[0].message.content.strip()\n",
        "\n",
        "# Hugging Face Inference API\n",
        "def hf_inference_generate(prompt: str, max_new_tokens=400) -> str:\n",
        "    if not os.getenv(\"HF_INFERENCE_TOKEN\") and not HF_INFERENCE_TOKEN:\n",
        "        raise RuntimeError(\"Set HF_INFERENCE_TOKEN to use HF Inference API.\")\n",
        "    url = f\"https://api-inference.huggingface.co/models/{HF_INFERENCE_MODEL}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {HF_INFERENCE_TOKEN or os.getenv('HF_INFERENCE_TOKEN')}\"}\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\"max_new_tokens\": max_new_tokens, \"temperature\": 0.2, \"return_full_text\": False}\n",
        "    }\n",
        "    r = requests.post(url, headers=headers, json=payload, timeout=60)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    # Response format can vary slightly; handle common cases\n",
        "    if isinstance(data, list) and data and \"generated_text\" in data[0]:\n",
        "        return data[0][\"generated_text\"].strip()\n",
        "    if isinstance(data, dict) and \"generated_text\" in data:\n",
        "        return data[\"generated_text\"].strip()\n",
        "    # Fallback\n",
        "    return str(data)\n"
      ],
      "metadata": {
        "id": "__ikGstXDsuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üß© RAG: Retrieve ‚Üí Augment Prompt ‚Üí Generate\n",
        "SYSTEM_INSTRUCTIONS = \"\"\"You answer strictly based on the provided context.\n",
        "If the answer isn't in the context, say you don't know. Be concise and accurate.\n",
        "\"\"\"\n",
        "\n",
        "def build_prompt(question:str, contexts:List[Dict]) -> str:\n",
        "    context_block = \"\\n\\n---\\n\".join(\n",
        "        [f\"[Chunk {i+1} | score={c['score']:.3f} | {c['source']}] {c['text']}\" for i,c in enumerate(contexts)]\n",
        "    )\n",
        "    prompt = f\"\"\"{SYSTEM_INSTRUCTIONS}\n",
        "\n",
        "Context:\n",
        "{context_block}\n",
        "\n",
        "User question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def generate_answer(question:str) -> Tuple[str, List[Dict], str]:\n",
        "    rets = retrieve(question, top_k=TOP_K)\n",
        "    prompt = build_prompt(question, rets)\n",
        "\n",
        "    if LLM_BACKEND == \"local_flan_t5\":\n",
        "        answer = local_generate(prompt)\n",
        "    elif LLM_BACKEND == \"groq\":\n",
        "        answer = groq_generate(prompt)\n",
        "    elif LLM_BACKEND == \"openai\":\n",
        "        answer = openai_generate(prompt)\n",
        "    elif LLM_BACKEND == \"hf_inference\":\n",
        "        answer = hf_inference_generate(prompt)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown LLM backend\")\n",
        "\n",
        "    return answer, rets, prompt\n"
      ],
      "metadata": {
        "id": "i8Dt5OVmDzhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üß™ Test once and capture artifacts for the report\n",
        "test_question = \"What is Retrieval Augmented Generation and how does it help here?\"\n",
        "answer, rets, prompt = generate_answer(test_question)\n",
        "\n",
        "print(\"QUESTION:\\n\", test_question, \"\\n\")\n",
        "print(\"RETRIEVAL RESULTS (top-k):\")\n",
        "for r in rets:\n",
        "    print(f\"- id={r['id']}, score={r['score']:.3f}, source={r['source']}\")\n",
        "print(\"\\nANSWER:\\n\", answer)\n",
        "\n",
        "# --- Create 'screenshots' as PNGs ---\n",
        "# 1) DB creation summary\n",
        "db_summary = f\"FAISS index created with {len(docs)} chunks using {EMBEDDING_MODEL}.\\nIndex path: {FAISS_INDEX_PATH}\\nMetadata path: {DB_META_JSON}\"\n",
        "png_db = ASSETS / \"db_creation.png\"\n",
        "text_to_image(db_summary, png_db, title=\"Database Creation & Insertion\")\n",
        "\n",
        "# 2) Retrieval results\n",
        "retrieval_text = \"Top-K Retrieval Results:\\n\\n\" + \"\\n\\n\".join(\n",
        "    [f\"[{i+1}] id={r['id']} score={r['score']:.3f} source={r['source']}\\n{wrap_for_image(r['text'], 100)}\"\n",
        "     for i, r in enumerate(rets)]\n",
        ")\n",
        "png_retrieval = ASSETS / \"retrieval.png\"\n",
        "text_to_image(retrieval_text, png_retrieval, title=\"Retrieval Results\")\n",
        "\n",
        "# 3) Final answer\n",
        "png_answer = ASSETS / \"final_answer.png\"\n",
        "text_to_image(f\"Question: {test_question}\\n\\nAnswer:\\n{answer}\", png_answer, title=\"Final Answer\")\n",
        "\n",
        "print(\"\\nSaved images for report:\")\n",
        "print(png_db)\n",
        "print(png_retrieval)\n",
        "print(png_answer)\n"
      ],
      "metadata": {
        "id": "RkEUmRs0D25y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üñ•Ô∏è Gradio app: Ask questions over your data\n",
        "def ui_answer(q):\n",
        "    if not q or not q.strip():\n",
        "        return \"\", \"\"\n",
        "    ans, rets, _ = generate_answer(q)\n",
        "    retrieved_str = \"\\n\\n\".join(\n",
        "        [f\"[{i+1}] score={r['score']:.3f} ({r['source']})\\n{r['text']}\" for i, r in enumerate(rets)]\n",
        "    )\n",
        "    return ans, retrieved_str\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# üîé RAG Demo (FAISS + Sentence-Transformers)\")\n",
        "    gr.Markdown(\"Ask questions based on your loaded data (text/PDF/URL).\")\n",
        "    with gr.Row():\n",
        "        inp = gr.Textbox(label=\"Your question\", placeholder=\"Ask something...\")\n",
        "    with gr.Row():\n",
        "        out = gr.Textbox(label=\"Generated answer\")\n",
        "        ctx = gr.Textbox(label=\"Retrieved chunks (for transparency)\")\n",
        "    btn = gr.Button(\"Ask\")\n",
        "    btn.click(fn=ui_answer, inputs=inp, outputs=[out, ctx])\n",
        "\n",
        "demo.launch(debug=False, share=False)\n"
      ],
      "metadata": {
        "id": "oqyTB1s3D7F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üìÑ Build a PDF report (with screenshots)\n",
        "tools_text = f\"\"\"\n",
        "Tools, Libraries & APIs Used\n",
        "- Embeddings: {EMBEDDING_MODEL} (Sentence-Transformers)\n",
        "- Vector DB: FAISS (cosine similarity via inner product on normalized vectors)\n",
        "- LLM Backend: {LLM_BACKEND}\n",
        "  - local_flan_t5 -> Transformers pipeline: {LOCAL_GENERATION_MODEL}\n",
        "  - groq -> {GROQ_MODEL}\n",
        "  - openai -> {OPENAI_MODEL}\n",
        "  - hf_inference -> {HF_INFERENCE_MODEL}\n",
        "- Parsing: pypdf for PDFs, BeautifulSoup+requests for scraping\n",
        "- UI: Gradio\n",
        "- Reporting: Pillow + ReportLab\n",
        "\"\"\"\n",
        "\n",
        "steps_text = \"\"\"\n",
        "Step-by-Step Process\n",
        "1) Data Loading: Accept raw text, PDF(s) (via upload), or small scraped pages (public URL).\n",
        "2) Chunking: Simple word-based chunking (~220 tokens with 40 overlap) to keep semantic units.\n",
        "3) Embeddings: Encode chunks with sentence-transformers 'all-MiniLM-L6-v2' (fast, free).\n",
        "4) Vector DB: Store embeddings in FAISS (IndexFlatIP) with normalized vectors ‚áí cosine similarity.\n",
        "5) Retrieval: For each query, embed the question and fetch Top-K similar chunks.\n",
        "6) Prompt Construction: Build a context block with retrieved chunks and a strict instruction.\n",
        "7) Generation: Send the prompt to the selected LLM backend; default local FLAN-T5 for no-API demo.\n",
        "8) UI: Gradio app to accept user questions, show retrieved chunks, and display the final answer.\n",
        "9) Report: Save images of DB creation summary, retrieval results, and final answer; compile to PDF.\n",
        "\"\"\"\n",
        "\n",
        "# Make PNG from steps & tools for inclusion\n",
        "png_tools = ASSETS / \"tools.png\"\n",
        "text_to_image(tools_text, png_tools, title=\"Tools, Libraries & APIs\")\n",
        "png_steps = ASSETS / \"steps.png\"\n",
        "text_to_image(steps_text, png_steps, title=\"Step-by-Step Explanation\")\n",
        "\n",
        "# Create PDF\n",
        "c = canvas.Canvas(REPORT_PDF, pagesize=A4)\n",
        "W, H = A4\n",
        "\n",
        "def add_img(path, title=None, y_offset=60):\n",
        "    c.setFont(\"Helvetica-Bold\", 14)\n",
        "    if title:\n",
        "        c.drawString(40, H-40, title)\n",
        "    img = ImageReader(str(path))\n",
        "    # Fit image to page width with margins\n",
        "    iw, ih = Image.open(path).size\n",
        "    max_w = W - 80\n",
        "    scale = min(max_w/iw, (H-120)/ih)\n",
        "    w, h = iw*scale, ih*scale\n",
        "    c.drawImage(img, 40, (H - 80 - h), width=w, height=h)\n",
        "    c.showPage()\n",
        "\n",
        "add_img(png_tools, \"Tools & Libraries\")\n",
        "add_img(png_steps, \"Process\")\n",
        "add_img(ASSETS / \"db_creation.png\", \"Database Creation & Insertion\")\n",
        "add_img(ASSETS / \"retrieval.png\", \"Retrieval Results\")\n",
        "add_img(ASSETS / \"final_answer.png\", \"Final Answer\")\n",
        "\n",
        "c.save()\n",
        "print(\"Report saved ->\", REPORT_PDF)\n"
      ],
      "metadata": {
        "id": "kzw-dHZDD-Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üîÅ Switch LLM backend on the fly (optional)\n",
        "def set_backend(name:str):\n",
        "    global LLM_BACKEND\n",
        "    LLM_BACKEND = name\n",
        "    print(\"LLM backend set to:\", LLM_BACKEND)\n",
        "\n",
        "# Example:\n",
        "# set_backend(\"groq\")         # requires GROQ_API_KEY\n",
        "# set_backend(\"openai\")       # requires OPENAI_API_KEY\n",
        "# set_backend(\"hf_inference\") # requires HF_INFERENCE_TOKEN\n",
        "# set_backend(\"local_flan_t5\")\n"
      ],
      "metadata": {
        "id": "zuZqijmsEBO_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}